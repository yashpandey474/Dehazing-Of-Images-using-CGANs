{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/yashpandey474/CSF425-Deep-Learning-Project-Task-2/blob/master/TASK2_TRAIN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pcd2C4DANr-T"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from database2 import DehazingDataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D6XXrqJqN7Vm"
   },
   "outputs": [],
   "source": [
    "# Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 3, 4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "#MODIFIED DISCRIMINATOR FOR W-LOSS; NO SIGMOID AT THE END\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(256, 1, 4, stride=2, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        # Perform global average pooling\n",
    "        x = torch.mean(x, dim=(2, 3))\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uSsmpF47N7mG"
   },
   "outputs": [],
   "source": [
    "root_dir = 'Task2Dataset'\n",
    "train_dir = os.path.join(root_dir, 'train')\n",
    "val_dir = os.path.join(root_dir, 'val')\n",
    "transform = transforms.Compose([\n",
    "                                #  transforms.Resize((224, 224)), # ASSUMING NO NEED FOR RESIZING AS ALL IMAGES ARE ALREADY 256*256\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize([0.5, 0.5, 0.5],[0.5, 0.5, 0.5])\n",
    "                                 ])\n",
    "\n",
    "train_dataset = DehazingDataset(train_dir, transform)\n",
    "val_dataset = DehazingDataset(val_dir, transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_bevMWacN7n-"
   },
   "outputs": [],
   "source": [
    "# Define the generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BblKBADFSNLQ"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define the loss function and optimizer [USING THE W-LOSS INSTEAD OF MIN-MAX LOSS]\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# W GANS RECOMMEND RMSPROP\n",
    "optimizer_D = optim.RMSprop(discriminator.parameters(), lr=0.00005)\n",
    "optimizer_G = optim.RMSprop(generator.parameters(), lr=0.00005)\n",
    "\n",
    "\n",
    "# THE CRITIC IS UPDATED MORE TIMES THAN GENERATOR FOR W-GANS\n",
    "n_critic = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ycMT2e9VSTnP",
    "outputId": "cb768911-24d4-4cbf-d1e0-9fdda3566c51"
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "epochs = []\n",
    "g_losses = []\n",
    "d_losses = []\n",
    "num_samples = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training the generator and discriminator\n",
    "    batch_no = 0\n",
    "    \n",
    "    for hazy_imgs, clean_imgs in tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{num_epochs}'):\n",
    "        # Training the discriminator\n",
    "        real_imgs = clean_imgs\n",
    "        fake_imgs = generator(hazy_imgs)\n",
    "        \n",
    "        for discr_train in range(n_critic):\n",
    "            real_outputs = discriminator(real_imgs)\n",
    "            fake_outputs = discriminator(fake_imgs.detach())\n",
    "            \n",
    "            # UPDATE THE DISCRIMINATOR [CRITIC]\n",
    "            optimizer_D.zero_grad()\n",
    "    \n",
    "            # WGAN utility, we ascend on this hence the loss will be the negative.\n",
    "            d_loss = -torch.mean(real_outputs - fake_outputs)\n",
    "    \n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "    \n",
    "            # CLIPPING OF THE DISCRIMINATOR WEIGHTS\n",
    "            for param in discriminator.parameters():\n",
    "                param.data.clamp_(-0.01, 0.01)\n",
    "\n",
    "        # UPDATE THE GENERATOR\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # REGENERATE IMAGES AND GET OUTPUTS FROM DISCRIMINATOR\n",
    "        fake_imgs = generator(hazy_imgs)\n",
    "        fake_outputs = discriminator(fake_imgs)\n",
    "\n",
    "        #  W-LOSS FOR GENERATOR\n",
    "        g_loss = -torch.mean(fake_outputs)\n",
    "        g_loss.backward()\n",
    "        \n",
    "        optimizer_G.step()\n",
    "\n",
    "        epochs.append(epoch + batch_no/len(train_dataloader))\n",
    "        g_losses.append(-g_loss.item()) # Negative because the loss is actually maximized in WGAN.\n",
    "        d_losses.append(-d_loss.item())\n",
    "\n",
    "        if batch_no % 30 == 0:\n",
    "            # Generate and display a few images\n",
    "            generated_images = generator(hazy_imgs[:num_samples]).detach().cpu()\n",
    "\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            for i in range(num_samples):\n",
    "                plt.subplot(2, num_samples, i + 1)\n",
    "                plt.imshow(hazy_imgs[i].permute(1, 2, 0))  # Assuming images are in CHW format\n",
    "                plt.title('Hazy Image')\n",
    "                plt.axis('off')\n",
    "\n",
    "                plt.subplot(2, num_samples, num_samples + i + 1)\n",
    "                plt.imshow(generated_images[i].permute(1, 2, 0))  # Assuming images are in CHW format\n",
    "                plt.title('Generated Image')\n",
    "                plt.axis('off')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "        batch_no += 1\n",
    "\n",
    "# Save the trained models\n",
    "torch.save(generator.state_dict(), 'generator_wloss.pth')\n",
    "torch.save(discriminator.state_dict(), 'discriminator_wloss.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ajYnAQb6S0UB",
    "outputId": "06f67958-ae7e-4021-ce2e-ac7f7ecf24a2"
   },
   "outputs": [],
   "source": [
    "real_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uwtngEyUSXb3",
    "outputId": "9d3c831b-0a2c-48aa-cc87-e4db1356e572"
   },
   "outputs": [],
   "source": [
    "real_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eNXLCKmrS8Of",
    "outputId": "70f370cf-9691-4561-eabc-bfe79b252365"
   },
   "outputs": [],
   "source": [
    "fake_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uIABBu14UEbo"
   },
   "outputs": [],
   "source": [
    "#output is 256*32 instead of 32 outputs"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOg0mdZRWtrPDuMO1Fbg8OC",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
