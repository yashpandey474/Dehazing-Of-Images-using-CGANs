{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "id": "b0497618-9387-4fa9-8ad8-f580c2373560"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models.segmentation import deeplabv3_resnet101\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.optim import lr_scheduler\n",
    "from database2 import DehazingDataset\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XAQ3zIRSq_xl",
    "outputId": "f5ff27cd-9f5c-4eeb-a3aa-7f9974c51984"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install opencv-contrib-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "id": "90946c71-c70f-4483-9273-9864bea60259"
   },
   "outputs": [],
   "source": [
    "# def estimate_transmission(image, window_size=15):\n",
    "#     # Convert the image to float32 format\n",
    "#     image = image.astype(np.float32) / 255.0\n",
    "\n",
    "#     # Compute the dark channel of the image\n",
    "#     dark_channel = np.min(image, axis=2)\n",
    "\n",
    "#     # Compute the minimum value in the dark channel using a local window\n",
    "#     kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (window_size, window_size))\n",
    "#     min_dark_channel = cv2.erode(dark_channel, kernel)\n",
    "\n",
    "#     # Estimate the atmospheric light\n",
    "#     atmospheric_light = np.percentile(image, 99, axis=(0, 1))\n",
    "\n",
    "#     # Compute the transmission map\n",
    "#     transmission = 1 - 0.95 * (dark_channel / min_dark_channel)\n",
    "\n",
    "#     # Clip transmission values to ensure they are within [0, 1]\n",
    "#     transmission = np.clip(transmission, 0, 1)\n",
    "\n",
    "#     return transmission, atmospheric_light\n",
    "\n",
    "def estimate_transmission_map(hazy_image, window_size=15, omega=0.95):\n",
    "    # Convert hazy image to numpy array\n",
    "    hazy_np = np.array(hazy_image)\n",
    "    \n",
    "    # Compute dark channel of hazy image\n",
    "    dark_channel = np.min(hazy_np, axis=2)\n",
    "    \n",
    "    # Estimate atmospheric light as the maximum intensity in the dark channel\n",
    "    atmospheric_light = np.max(dark_channel)\n",
    "    \n",
    "    # Normalize dark channel\n",
    "    normalized_dark_channel = dark_channel / atmospheric_light\n",
    "    \n",
    "    # Compute transmission map using the atmospheric light and omega parameter\n",
    "    transmission_map = 1 - omega * normalized_dark_channel\n",
    "    \n",
    "    # Apply guided filter for refinement (optional but recommended)\n",
    "    # You may need to install the 'cv2' library for this step\n",
    "    \n",
    "    transmission_map = cv2.ximgproc.guidedFilter(hazy_np.astype(np.float32), transmission_map.astype(np.float32), radius=window_size, eps=1e-3)\n",
    "    \n",
    "    return transmission_map, -1\n",
    "\n",
    "def generate_transmission_maps(hazy_images, window_size=15):\n",
    "    transmission_maps = []\n",
    "    atmospheric_lights = []\n",
    "\n",
    "    for hazy_image in hazy_images:\n",
    "        \n",
    "        transmission, atmospheric_light = estimate_transmission_map(hazy_image, window_size)\n",
    "        transmission_maps.append(transmission)\n",
    "        atmospheric_lights.append(atmospheric_light)\n",
    "\n",
    "    return transmission_maps, atmospheric_lights\n",
    "\n",
    "def visualize_transmission_maps(transmission_maps):\n",
    "    num_images = len(transmission_maps)\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(15, 5))\n",
    "    for i, transmission_map in enumerate(transmission_maps):\n",
    "        axes[i].imshow(transmission_map, cmap='gray')\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(f\"Transmission Map {i+1}\")\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "id": "od0V2BwHvAyp"
   },
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "id": "1AxXBAmYvCCq"
   },
   "outputs": [],
   "source": [
    "root_dir = '../Task2Dataset'\n",
    "train_dir = os.path.join(root_dir, 'train')\n",
    "val_dir = os.path.join(root_dir, 'val')\n",
    "\n",
    "# DIFFERENT TRANSFORM [USED BY DEEPLAB]\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((512, 512)),  # Resize input image to match model's input size\n",
    "#     transforms.ToTensor(),           # Convert PIL image to tensor\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize pixel values\n",
    "# ])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),   \n",
    "    transforms.Normalize(mean = [0.5, 0.5, 0.5],std = [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "train_dataset = DehazingDataset(train_dir, transform)\n",
    "val_dataset = DehazingDataset(val_dir, transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_hazy_transmission_pairs(hazy_images, transmission_maps):\n",
    "    num_images = len(hazy_images)\n",
    "    num_pairs = num_images // 4\n",
    "    num_rows = num_pairs * 2  # Each pair will occupy two rows\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, 4, figsize=(20, 4*num_rows))\n",
    "    \n",
    "    for i in range(num_pairs):\n",
    "        for j in range(4):\n",
    "            hazy_idx = i * 4 + j\n",
    "            map_idx = i * 4 + j\n",
    "            if hazy_idx < num_images:\n",
    "                axes[i*2, j].imshow(hazy_images[hazy_idx])\n",
    "                axes[i*2, j].set_title(f\"Hazy Image {hazy_idx+1}\")\n",
    "                axes[i*2, j].axis('off')\n",
    "                \n",
    "                if map_idx < len(transmission_maps):\n",
    "                    axes[i*2+1, j].imshow(transmission_maps[map_idx], cmap='gray')\n",
    "                    axes[i*2+1, j].set_title(f\"Transmission Map {map_idx+1}\")\n",
    "                    axes[i*2+1, j].axis('off')\n",
    "                \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(image):\n",
    "    \"\"\"Denormalize a tensor image.\"\"\"\n",
    "    mean = np.array([0.5, 0.5, 0.5])\n",
    "    std = np.array([0.5, 0.5, 0.5])\n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "    image = std * image + mean\n",
    "    image = np.clip(image, 0, 1)\n",
    "    return image\n",
    "    \n",
    "hazy_images_list = []\n",
    "transmission_maps_list = []\n",
    "\n",
    "num_images = 0\n",
    "for hazy_imgs, clean_imgs in train_dataloader:\n",
    "    # CREATE TRANSMISSION MAPS\n",
    "    hazy_imgs = hazy_imgs[:5]  # Assuming you want to process the first 5 hazy images only\n",
    "\n",
    "    denormalized_imgs = [denormalize(img) for img in hazy_imgs]\n",
    "\n",
    "    transmission_maps, _ = generate_transmission_maps(denormalized_imgs)\n",
    "\n",
    "    # VISUALIZE TRANSMISSION MAPS IN ROWS OF 5\n",
    "    transmission_maps_list.extend(transmission_maps)\n",
    "    hazy_images_list.extend(denormalized_imgs)\n",
    "\n",
    "\n",
    "    num_images += 5\n",
    "\n",
    "    if num_images >= 200:\n",
    "        break\n",
    "\n",
    "visualize_hazy_transmission_pairs(hazy_images_list, transmission_maps_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "id": "3e45a0d4-5e8d-40e9-91fd-4bb24e50c95b"
   },
   "outputs": [],
   "source": [
    "# Load the pre-trained semantic segmentation model\n",
    "model = deeplabv3_resnet101(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Define transformations for pre-processing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),  # Resize input image to match model's input size\n",
    "    transforms.ToTensor(),           # Convert PIL image to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize pixel values\n",
    "])\n",
    "\n",
    "# Load and pre-process the hazy image\n",
    "import time\n",
    "for i in range(500):\n",
    "    hazy_image_path = hazy_images_paths[i]\n",
    "    hazy_image = Image.open(hazy_image_path)\n",
    "    hazy_image = transform(hazy_image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Perform semantic segmentation inference\n",
    "    with torch.no_grad():\n",
    "        output = model(hazy_image)['out'][0]\n",
    "\n",
    "    # Convert output tensor to numpy array\n",
    "    semantic_map = torch.argmax(output, dim=0).numpy()\n",
    "\n",
    "    # Visualize the segmented semantic map\n",
    "    plt.imshow(semantic_map)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {
    "id": "y6jTFhf5vg74"
   },
   "source": [
    "# View images from dataloader in rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "46f95387-ce30-46ac-85cc-a13e80cd1ec2",
    "outputId": "5ca9c9f3-aee0-4061-edfc-12f22f5c7bdb"
   },
   "outputs": [],
   "source": [
    "model = deeplabv3_resnet101(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "num_images = 200\n",
    "count = 0\n",
    "fig, axes = plt.subplots(num_images // 5, 5, figsize=(20, 4*num_images // 5))\n",
    "\n",
    "# for hazy images\n",
    "for images, clean_imgs in train_dataloader:\n",
    "    # Perform semantic segmentation inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)['out']\n",
    "        segmentation_maps = torch.argmax(outputs, dim=1).numpy()\n",
    "\n",
    "    # Visualize segmentation maps in rows of 5 images\n",
    "    for i in range(images.size(0)):\n",
    "        ax = axes[count // 5, count % 5]\n",
    "        ax.imshow(segmentation_maps[i], cmap='viridis')\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f'Image {count+1}')\n",
    "        count += 1\n",
    "        if count >= num_images:\n",
    "            break\n",
    "    if count >= num_images:\n",
    "        break\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "id": "TsXDWaOxvlvE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
