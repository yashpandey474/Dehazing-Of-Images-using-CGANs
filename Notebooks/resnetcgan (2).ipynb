{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pcd2C4DANr-T"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import time\n",
    "from torchvision.transforms import Lambda\n",
    "from torch.optim import lr_scheduler\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IKB1fGCgb1MV",
    "outputId": "30149da2-c278-4e78-e7be-a489f709c86a"
   },
   "outputs": [],
   "source": [
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ingvJgD4vCb"
   },
   "source": [
    "# U-Net architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DPT5yiw-4vCf"
   },
   "outputs": [],
   "source": [
    "def down_conv(in_channels, out_channels, kernel_size, stride, padding):\n",
    "  conv = nn.Sequential(\n",
    "      nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "      nn.BatchNorm2d(out_channels),\n",
    "      nn.LeakyReLU(0.2, inplace=True)\n",
    "  )\n",
    "  return conv\n",
    "\n",
    "def up_conv(in_channels, out_channels, kernel_size, stride, padding):\n",
    "  conv = nn.Sequential(\n",
    "      nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "      nn.BatchNorm2d(out_channels),\n",
    "      nn.ReLU(inplace=True)\n",
    "  )\n",
    "  return conv\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        # Encoding layers\n",
    "        self.down_conv_1 = down_conv(3,64,5,1,2)\n",
    "        self.down_conv_2 = down_conv(64,128,4,2,1)\n",
    "        self.down_conv_3 = down_conv(128,256,4,2,1)\n",
    "        self.down_conv_4 = down_conv(256,512,4,2,1)\n",
    "\n",
    "        self.up_trans_3 = up_conv(256, 128, 4, 2, 1)\n",
    "        self.up_conv_3 = down_conv(256, 128, 3, 1, 1)\n",
    "        self.up_trans_4 = up_conv(128, 64, 4, 2, 1)\n",
    "        self.up_conv_4 = down_conv(128, 64, 3, 1, 1)\n",
    "\n",
    "        self.out = nn.Conv2d(64, 3, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, image):\n",
    "      #encoder\n",
    "                                   # image = [32, 3, 256, 256]\n",
    "      x1 = self.down_conv_1(image) # x1 = [32, 64, 256, 256]\n",
    "      x2 = self.down_conv_2(x1)    # x2 = [32, 128, 128, 128]\n",
    "      x3 = self.down_conv_3(x2)    # x3 = [32, 256, 64, 64]\n",
    "\n",
    "      #decoder\n",
    "      y = self.up_trans_3(x3)                   # y3 = [32, 128, 128, 128]\n",
    "      y = self.up_conv_3(torch.cat([y,x2],1))  # y3 = [32, 128, 128, 128]\n",
    "      y = self.up_trans_4(y)                   # y4 = [32, 64, 256, 256]\n",
    "      y = self.up_conv_4(torch.cat([y,x1],1))  # y4 = [32, 64, 256, 256]\n",
    "      y = self.out(y)\n",
    "\n",
    "      return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yXfd9d27LR2P",
    "outputId": "9945a8e9-0396-4b6d-90ee-96694ea2d096"
   },
   "outputs": [],
   "source": [
    "image = torch.rand((32,3,256,256))\n",
    "model = Generator()\n",
    "output = model(image)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rrCH3NZ04vCf"
   },
   "source": [
    "# Discriminator from repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HhOMX3nR4vCg"
   },
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self,in_channels, out_channels, stride = 2):\n",
    "        super(CNNBlock,self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels,out_channels,4,stride,padding_mode='reflect',bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,in_channels = 3, features = [64,128,256,512]):\n",
    "        super().__init__()\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(in_channels*2,features[0],kernel_size=4,stride=2,padding=1,padding_mode='reflect'),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        ) # according to paper 64 channel doesn't contain BatchNorm2d\n",
    "        layers = []\n",
    "        in_channels = features[0]\n",
    "        for feature in features[1:]:\n",
    "            layers.append(CNNBlock(in_channels,feature,stride=1 if feature==features[-1] else 2 ))\n",
    "            in_channels = feature\n",
    "\n",
    "        layers.append(\n",
    "            nn.Conv2d(in_channels,1,kernel_size=4,stride=1,padding=1,padding_mode='reflect')\n",
    "        )\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self,x,y):\n",
    "        x = torch.cat([x,y],dim=1)\n",
    "        x = self.initial(x)\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5i7BDYEl0JRQ",
    "outputId": "708b219c-e546-4877-9173-c34ab4427fe0"
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    x = torch.randn((1, 3, 256, 256))\n",
    "    y = torch.randn((1, 3, 256, 256))\n",
    "    model = Discriminator(in_channels=3)\n",
    "    preds = model(x, y)\n",
    "    print(model)\n",
    "    print(preds.shape)\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6xAPa638QBbF"
   },
   "outputs": [],
   "source": [
    "class DehazingDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        #Get the images\n",
    "        self.root_dir = root_dir\n",
    "        hazy_images_path = os.path.join(root_dir, 'hazy')\n",
    "        clean_images_path = os.path.join(root_dir, 'GT')\n",
    "\n",
    "\n",
    "        self.hazy_images = sorted([os.path.join(hazy_images_path,f) for f in os.listdir(hazy_images_path) if  f.endswith('.jpg') or f.endswith('.png') or f.endswith('.jpeg')])\n",
    "        self.clean_images = sorted([os.path.join(clean_images_path, f) for f in os.listdir(clean_images_path) if f.endswith('.jpg') or f.endswith('.png') or f.endswith('.jpeg')])\n",
    "\n",
    "        #Filter the images to ensure they are counterparts of the same scene\n",
    "        self.size = len(self.hazy_images)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        hazy_img = self.rgb_loader(self.hazy_images[index])\n",
    "        clean_img = self.rgb_loader(self.clean_images[index])\n",
    "        hazy_img = self.transform(hazy_img)\n",
    "        clean_img = self.transform(clean_img)\n",
    "        return hazy_img, clean_img\n",
    "\n",
    "    def rgb_loader(self, path):\n",
    "        with open(path, 'rb') as f:\n",
    "            img = Image.open(f)\n",
    "            return img.convert('RGB')\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ny3FPPrk4vCj"
   },
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uSsmpF47N7mG"
   },
   "outputs": [],
   "source": [
    "root_dir = train_dir = \"/content/drive/My Drive/Task2Dataset\"\n",
    "train_dir = os.path.join(root_dir, 'train')\n",
    "val_dir = os.path.join(root_dir, 'val')\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize images\n",
    "    transforms.ToTensor(),           # Convert to tensor\n",
    "    Lambda(lambda x: x / torch.max(x.abs()))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JwahtDv_AF6S"
   },
   "outputs": [],
   "source": [
    "train_dataset = DehazingDataset(train_dir, transform)\n",
    "# val_dataset = DehazingDataset(val_dir, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83AA0byzAF6S"
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9riCnvf4vCk"
   },
   "source": [
    "# Function to show images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rEOd9Uzv4vCk"
   },
   "outputs": [],
   "source": [
    "def show_images(hazy_imgs, clean_imgs, generated_imgs, num_images=5):\n",
    "    fig, axes = plt.subplots(3, num_images, figsize=(15, 10))\n",
    "    for i in range(num_images):\n",
    "        clean_image = clean_imgs[i].detach().permute(1, 2, 0).cpu().numpy()\n",
    "        hazy_image = hazy_imgs[i].detach().permute(1, 2, 0).cpu().numpy()\n",
    "        generated_image = generated_imgs[i].detach().permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "\n",
    "        # Plot hazy images\n",
    "        axes[0, i].imshow(hazy_image)\n",
    "        axes[0, i].axis('off')\n",
    "        axes[0, i].set_title(\"Hazy Image\")\n",
    "\n",
    "        # Plot clean images\n",
    "        axes[1, i].imshow(clean_image)\n",
    "        axes[1, i].axis('off')\n",
    "        axes[1, i].set_title(\"Clean Image\")\n",
    "\n",
    "        # Plot generated images\n",
    "        axes[2, i].imshow(generated_image)\n",
    "        axes[2, i].axis('off')\n",
    "        axes[2, i].set_title(\"Generated Image\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oArUbSwH4vCo"
   },
   "source": [
    "# Initialise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qFgJJP2GAXeO",
    "outputId": "759dbcf6-812a-4f8b-eeb1-71132a88cb1e"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('GPU:', torch.cuda.get_device_name(0))  # 0 is the GPU index, change if you have multiple GPUs\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('Using CPU for computations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_bevMWacN7n-"
   },
   "outputs": [],
   "source": [
    "# Define the generator and discriminator\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BblKBADFSNLQ"
   },
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "bce_loss = nn.BCEWithLogitsLoss()\n",
    "l1_loss = nn.L1Loss()\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GCfXFtwOwnvT"
   },
   "outputs": [],
   "source": [
    "lr_step_size = 2\n",
    "lr_gamma = 0.5\n",
    "\n",
    "schedulerG = lr_scheduler.StepLR(optimizer_G, lr_step_size, lr_gamma)\n",
    "schedulerD = lr_scheduler.StepLR(optimizer_D, lr_step_size, lr_gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8v-yQAwS4vCs"
   },
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ycMT2e9VSTnP",
    "outputId": "2cea4d6c-b7fc-4e9d-98dc-190994081180"
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    batch_no = 0\n",
    "    schedulerG.step()\n",
    "    schedulerD.step()\n",
    "\n",
    "    # Initialize total losses for the epoch\n",
    "    g_total_loss = 0\n",
    "    d_total_loss = 0\n",
    "\n",
    "    # Training the generator and discriminator\n",
    "    for hazy_imgs, clean_imgs in tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{num_epochs}'):\n",
    "\n",
    "        hazy_imgs = hazy_imgs.to(device)\n",
    "        clean_imgs = clean_imgs.to(device)\n",
    "\n",
    "        # Training the discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "        real_imgs = clean_imgs\n",
    "        real_imgs = real_imgs.to(device)\n",
    "\n",
    "        # GENERATOR TAKES HAZY IMAGES AS INPUT\n",
    "        fake_imgs = generator(hazy_imgs)\n",
    "        fake_imgs = fake_imgs.to(device)\n",
    "\n",
    "        # PREDICTIONS OF DISCRIMINATOR FOR REAL IMAGES\n",
    "        real_outputs = discriminator(hazy_imgs, real_imgs)\n",
    "\n",
    "        # PREDICTIONS OF DISCRIMINATOR FOR FAKE IMAGES\n",
    "        fake_outputs = discriminator(hazy_imgs, fake_imgs.detach())\n",
    "\n",
    "        # CREATE LABELS FOR LOSS CALCULATION\n",
    "        real_labels = torch.ones_like(real_outputs).to(device)\n",
    "        fake_labels = torch.zeros_like(fake_outputs).to(device)\n",
    "\n",
    "        d_loss_real = bce_loss(real_outputs, real_labels)\n",
    "        d_loss_fake = bce_loss(fake_outputs, fake_labels)\n",
    "        d_loss = (d_loss_real + d_loss_fake)/2\n",
    "\n",
    "        # Update discriminator\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Accumulate discriminator loss\n",
    "        d_total_loss += d_loss.item()\n",
    "\n",
    "        # Training the generator\n",
    "        optimizer_G.zero_grad()\n",
    "        fake_imgs = generator(hazy_imgs)\n",
    "        fake_imgs = fake_imgs.to(device)\n",
    "        fake_outputs = discriminator(hazy_imgs, fake_imgs)\n",
    "        g_loss = bce_loss(fake_outputs, real_labels)\n",
    "\n",
    "        # Compute reconstruction loss\n",
    "        g_res_loss = l1_loss(fake_imgs, clean_imgs)\n",
    "\n",
    "        # Update generator\n",
    "        g_complete_loss = g_loss + g_res_loss\n",
    "        g_complete_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # Accumulate generator loss\n",
    "        g_total_loss += g_complete_loss.item()\n",
    "\n",
    "        batch_no += 1\n",
    "\n",
    "        # Clear GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Display images every 20 batches\n",
    "        if batch_no % 20 == 0:\n",
    "            show_images(hazy_imgs, clean_imgs, fake_imgs, num_images=5)\n",
    "\n",
    "    # Print average losses for the epoch\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Generator Loss: {g_total_loss / len(train_dataloader):.4f}, Discriminator Loss: {d_total_loss / len(train_dataloader):.4f}\")\n",
    "\n",
    "    # Save the trained models after each epoch\n",
    "    torch.save(generator.state_dict(), f'generator_epoch_{epoch + 1}.pth')\n",
    "    torch.save(discriminator.state_dict(), f'discriminator_epoch_{epoch + 1}.pth')\n",
    "\n",
    "# Save the trained models\n",
    "torch.save(generator.state_dict(), 'generator_l1_cgan.pth')\n",
    "torch.save(discriminator.state_dict(), 'discriminator_l1_cgan.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bYUo_AzTKJa0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
