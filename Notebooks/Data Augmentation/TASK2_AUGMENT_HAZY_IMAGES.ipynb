{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from database2 import DehazingDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Functions for augmenting hazy images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_haze(image):\n",
    "    # Convert image to uint8 data type\n",
    "    image_uint8 = (image * 255).astype(np.uint8)\n",
    "    \n",
    "    # Simulate haze by blending the image with a white overlay\n",
    "    overlay = np.full_like(image_uint8, (255, 255, 255), dtype=np.uint8)  # White overlay\n",
    "    alpha = random.uniform(0.1, 0.5)  # Random transparency level\n",
    "    haze_image = cv2.addWeighted(image_uint8, 1 - alpha, overlay, alpha, 0)\n",
    "    \n",
    "    return haze_image\n",
    "\n",
    "def add_haze_gaussian(image):\n",
    "    # Apply Gaussian blur to the image\n",
    "    blurred_image = cv2.GaussianBlur(image, (15, 15), 0)\n",
    "    \n",
    "    # Simulate haze by blending the original image with the blurred image\n",
    "    alpha = random.uniform(0.1, 0.5)  # Random transparency level\n",
    "    haze_image = cv2.addWeighted(image, 1 - alpha, blurred_image, alpha, 0)\n",
    "    return haze_image\n",
    "\n",
    "def add_noise(image):\n",
    "    # Generate Gaussian noise\n",
    "    mean = 0\n",
    "    sigma = random.uniform(10, 30)  # Random standard deviation for noise\n",
    "    noise = np.random.normal(mean, sigma, image.shape).astype(image.dtype)  # Ensure same data type as image\n",
    "    \n",
    "    # Add noise to the image\n",
    "    noisy_image = cv2.add(image, noise)\n",
    "    return noisy_image\n",
    "\n",
    "def add_atmospheric_scattering(image, depth_map=None):\n",
    "    # Simulate atmospheric scattering by attenuating light based on depth\n",
    "    if depth_map is None:\n",
    "        depth_map = np.ones_like(image, dtype=np.float32)  # Default to uniform depth map\n",
    "    \n",
    "    # Define parameters for scattering\n",
    "    scattering_coefficient = random.uniform(0.1, 0.2)  # Random scattering coefficient\n",
    "    light_intensity = random.uniform(1, 3.0)  # Random light intensity\n",
    "    \n",
    "    # Simulate scattering effect\n",
    "    scattered_image = image * np.exp(-scattering_coefficient * depth_map * light_intensity)\n",
    "    \n",
    "    return scattered_image\n",
    "\n",
    "def augment_hazy_image(image):\n",
    "    # Apply a combination of augmentation techniques\n",
    "    augmented_image = add_haze(image)\n",
    "    augmented_image = add_haze_gaussian(augmented_image)\n",
    "    augmented_image = add_noise(augmented_image)\n",
    "    augmented_image = add_atmospheric_scattering(augmented_image)\n",
    "    \n",
    "    return augmented_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Add haze & random occlusion are useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_haze(image):\n",
    "    # Convert image to uint8 data type\n",
    "    image_uint8 = (image * 255).astype(np.uint8)\n",
    "    \n",
    "    # Simulate haze by blending the image with a white overlay\n",
    "    overlay = np.full_like(image_uint8, (255, 255, 255), dtype=np.uint8)  # White overlay\n",
    "    alpha = random.uniform(0.1, 0.5)  # Random transparency level\n",
    "    haze_image = cv2.addWeighted(image_uint8, 1 - alpha, overlay, alpha, 0)\n",
    "    \n",
    "    return haze_image\n",
    "\n",
    "\n",
    "def add_random_occlusion(image):\n",
    "    # Randomly generate occlusions (black patches) in the image\n",
    "    occlusion_mask = np.random.choice([0, 1], size=image.shape[:2], p=[0.45, 0.55])  # 5% chance of occlusion\n",
    "    occluded_image = image.copy()\n",
    "    occluded_image[occlusion_mask.astype(bool)] = 0\n",
    "    return occluded_image\n",
    "\n",
    "\n",
    "def apply_gradient_overlay(image):\n",
    "    # Generate a linear gradient overlay\n",
    "    gradient = np.linspace(0, 1, image.shape[0])\n",
    "    gradient_overlay = (gradient[:, np.newaxis] * np.ones((image.shape[1], 3))).astype(np.uint8)\n",
    "    \n",
    "    # Resize the gradient overlay to match the size of the input image\n",
    "    gradient_overlay_resized = cv2.resize(gradient_overlay, (image.shape[1], image.shape[0]))\n",
    "    \n",
    "    # Check the dimensions of both images before blending\n",
    "    print(\"Image shape:\", image.shape)\n",
    "    print(\"Gradient overlay shape:\", gradient_overlay_resized.shape)\n",
    "    \n",
    "    return cv2.addWeighted(image, 0.5, gradient_overlay_resized, 0.5, 0)\n",
    "\n",
    "def add_random_lighting(image):\n",
    "    # Simulate random lighting effects\n",
    "    brightness = random.uniform(0.5, 1.5)  # Random brightness adjustment\n",
    "    contrast = random.uniform(0.5, 1.5)  # Random contrast adjustment\n",
    "    lightened_image = cv2.convertScaleAbs(image, alpha=contrast, beta=brightness)\n",
    "    return lightened_image\n",
    "\n",
    "def apply_color_distortion(image):\n",
    "    # Simulate color distortion or tinting\n",
    "    red_scale = random.uniform(0.8, 1.2)  # Random scaling factor for red channel\n",
    "    green_scale = random.uniform(0.8, 1.2)  # Random scaling factor for green channel\n",
    "    blue_scale = random.uniform(0.8, 1.2)  # Random scaling factor for blue channel\n",
    "    distorted_image = image * np.array([blue_scale, green_scale, red_scale])\n",
    "    return np.clip(distorted_image, 0, 255).astype(np.uint8)\n",
    "\n",
    "def apply_motion_blur(image):\n",
    "    # Simulate motion blur\n",
    "    kernel_size = random.choice([3, 5, 7])  # Random kernel size for blur\n",
    "    motion_blur_kernel = np.zeros((kernel_size, kernel_size))\n",
    "    motion_blur_kernel[int((kernel_size - 1) / 2), :] = np.ones(kernel_size)\n",
    "    motion_blur_kernel /= kernel_size\n",
    "    return cv2.filter2D(image, -1, motion_blur_kernel)\n",
    "\n",
    "\n",
    "def add_random_black_clouds(image):\n",
    "    # Randomly generate black clouds (dark patches) in the image\n",
    "    cloud_mask = np.random.choice([0, 1], size=image.shape[:2], p=[0.2, 0.8])  # 20% chance of cloud\n",
    "    cloud_image = image.copy()\n",
    "    cloud_image[cloud_mask.astype(bool)] = 0\n",
    "    return cloud_image\n",
    "\n",
    "def augment_hazy_image(image):\n",
    "    # Apply a combination of augmentation techniques\n",
    "    augmented_image = add_haze(image)\n",
    "    augmented_image = add_random_occlusion(augmented_image)\n",
    "    augmented_image = apply_gradient_overlay(augmented_image)\n",
    "    augmented_image = add_random_lighting(augmented_image)\n",
    "    augmented_image = apply_color_distortion(augmented_image)\n",
    "    augmented_image = apply_motion_blur(augmented_image)\n",
    "    return augmented_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Functions from https://www.freecodecamp.org/news/image-augmentation-make-it-rain-make-it-snow-how-to-modify-a-photo-with-machine-learning-163c0cb3843f/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_brightness(image):\n",
    "    image_HLS = cv2.cvtColor(image,cv2.COLOR_RGB2HLS)\n",
    "    ## Conversion to HLS\n",
    "    image_HLS = np.array(image_HLS, dtype = np.float64)\n",
    "    random_brightness_coefficient = np.random.uniform()+0.5 ## generates value between 0.5 and 1.5\n",
    "    image_HLS[:,:,1] = image_HLS[:,:,1]*random_brightness_coefficient ## scale pixel values up or down for channel 1(Lightness)\n",
    "    image_HLS[:,:,1][image_HLS[:,:,1]>255]  = 255 ##Sets all values above 255 to 255\n",
    "    image_HLS = np.array(image_HLS, dtype = np.uint8)\n",
    "    image_RGB = cv2.cvtColor(image_HLS,cv2.COLOR_HLS2RGB)\n",
    "    ## Conversion to RGB\n",
    "    return image_RGB\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def generate_shadow_coordinates(imshape, no_of_shadows=1):\n",
    "    vertices_list = []\n",
    "    for index in range(no_of_shadows):\n",
    "        vertex = []\n",
    "        for dimensions in range(np.random.randint(3, 15)):  # Dimensionality of the shadow polygon\n",
    "            vertex.append((imshape[1] * np.random.uniform(), imshape[0] // 3 + imshape[0] * np.random.uniform()))\n",
    "        vertices = np.array([vertex], dtype=np.int32)  # Single shadow vertices\n",
    "        vertices_list.append(vertices)\n",
    "    return vertices_list  # List of shadow vertices\n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def add_shadow(image, no_of_shadows=1):\n",
    "    image_HLS = cv2.cvtColor(image, cv2.COLOR_RGB2HLS)  # Conversion to HLS\n",
    "    mask = np.zeros_like(image)\n",
    "    imshape = image.shape\n",
    "    vertices_list = generate_shadow_coordinates(imshape, no_of_shadows)  # Getting list of shadow vertices\n",
    "    \n",
    "    for vertices in vertices_list:\n",
    "        cv2.fillPoly(mask, vertices, 255)  # Adding all shadow polygons on empty mask, single 255 denotes only red channel\n",
    "        image_HLS[:,:,1][mask[:,:,0] == 255] = image_HLS[:,:,1][mask[:,:,0] == 255] * 0.5  # If red channel is hot, image's \"Lightness\" channel's brightness is lowered\n",
    "    \n",
    "    image_RGB = cv2.cvtColor(image_HLS, cv2.COLOR_HLS2RGB)  # Conversion to RGB\n",
    "    return image_RGB\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def add_snow(image):\n",
    "    image_HLS = cv2.cvtColor(image, cv2.COLOR_RGB2HLS)  # Conversion to HLS\n",
    "    image_HLS = np.array(image_HLS, dtype=np.float64)\n",
    "    brightness_coefficient = 2.5\n",
    "    snow_point = 140  # Increase this for more snow\n",
    "    \n",
    "    # Scale pixel values up for channel 1 (Lightness)\n",
    "    image_HLS[:,:,1][image_HLS[:,:,1] < snow_point] = image_HLS[:,:,1][image_HLS[:,:,1] < snow_point] * brightness_coefficient\n",
    "    \n",
    "    # Set all values above 255 to 255\n",
    "    image_HLS[:,:,1][image_HLS[:,:,1] > 255] = 255\n",
    "    \n",
    "    image_HLS = np.array(image_HLS, dtype=np.uint8)\n",
    "    image_RGB = cv2.cvtColor(image_HLS, cv2.COLOR_HLS2RGB)  # Conversion to RGB\n",
    "    return image_RGB\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def generate_random_lines(imshape, slant, drop_length):\n",
    "    drops = []\n",
    "    for i in range(1500):  # If you want heavy rain, try increasing this\n",
    "        if slant < 0:\n",
    "            x = np.random.randint(slant, imshape[1])\n",
    "        else:\n",
    "            x = np.random.randint(0, imshape[1] - slant)\n",
    "        y = np.random.randint(0, imshape[0] - drop_length)\n",
    "        drops.append((x, y))\n",
    "    return drops\n",
    "\n",
    "def add_rain(image):\n",
    "    imshape = image.shape\n",
    "    slant_extreme = 10\n",
    "    slant = np.random.randint(-slant_extreme, slant_extreme)\n",
    "    drop_length = 20\n",
    "    drop_width = 2\n",
    "    drop_color = (200, 200, 200)  # A shade of gray\n",
    "    rain_drops = generate_random_lines(imshape, slant, drop_length)\n",
    "    \n",
    "    for rain_drop in rain_drops:\n",
    "        cv2.line(image, (rain_drop[0], rain_drop[1]), (rain_drop[0] + slant, rain_drop[1] + drop_length), drop_color, drop_width)\n",
    "    \n",
    "    image = cv2.blur(image, (7, 7))  # Rainy views are blurry\n",
    "    \n",
    "    brightness_coefficient = 0.7  # Rainy days are usually shady\n",
    "    image_HLS = cv2.cvtColor(image, cv2.COLOR_RGB2HLS)  # Conversion to HLS\n",
    "    image_HLS[:,:,1] = image_HLS[:,:,1] * brightness_coefficient  # Scale pixel values down for channel 1 (Lightness)\n",
    "    image_RGB = cv2.cvtColor(image_HLS, cv2.COLOR_HLS2RGB)  # Conversion to RGB\n",
    "    \n",
    "    return image_RGB\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def add_blur(image, x, y, hw):\n",
    "    image[y:y+hw, x:x+hw, 1] = image[y:y+hw, x:x+hw, 1] + 1\n",
    "    image[:,:,1][image[:,:,1] > 255] = 255  # Sets all values above 255 to 255\n",
    "    image[y:y+hw, x:x+hw, 1] = cv2.blur(image[y:y+hw, x:x+hw, 1], (10, 10))\n",
    "    return image\n",
    "\n",
    "def generate_random_blur_coordinates(imshape, hw):\n",
    "    blur_points = []\n",
    "    midx = imshape[1] // 2 - hw - 100\n",
    "    midy = imshape[0] // 2 - hw - 100\n",
    "    index = 1\n",
    "    while midx > -100 or midy > -100:  # Radially generating coordinates\n",
    "        for i in range(250 * index):\n",
    "            x = np.random.randint(midx, imshape[1] - midx - hw)\n",
    "            y = np.random.randint(midy, imshape[0] - midy - hw)\n",
    "            blur_points.append((x, y))\n",
    "        midx -= 250 * imshape[1] // sum(imshape)\n",
    "        midy -= 250 * imshape[0] // sum(imshape)\n",
    "        index += 1\n",
    "    return blur_points\n",
    "\n",
    "def add_fog(image):\n",
    "    image_HLS = cv2.cvtColor(image, cv2.COLOR_RGB2HLS)  # Conversion to HLS\n",
    "    mask = np.zeros_like(image)\n",
    "    imshape = image.shape\n",
    "    hw = 100\n",
    "    image_HLS[:,:,1] = image_HLS[:,:,1] * 0.8\n",
    "    haze_list = generate_random_blur_coordinates(imshape, hw)\n",
    "    \n",
    "    for haze_points in haze_list:\n",
    "        image_HLS[:,:,1][image_HLS[:,:,1] > 255] = 255  # Sets all values above 255 to 255\n",
    "        image_HLS = add_blur(image_HLS, haze_points[0], haze_points[1], hw)\n",
    "    \n",
    "    image_RGB = cv2.cvtColor(image_HLS, cv2.COLOR_HLS2RGB)  # Conversion to RGB\n",
    "    return image_RGB\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing your dataset\n",
    "dataset_dir = \"Task2Dataset\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 'Task2Dataset'\n",
    "train_dir = os.path.join(root_dir, 'train')\n",
    "val_dir = os.path.join(root_dir, 'val')\n",
    "transform = transforms.Compose([\n",
    "                                #  transforms.Resize((224, 224)), # ASSUMING NO NEED FOR RESIZING AS ALL IMAGES ARE ALREADY 256*256\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
    "                                 ])\n",
    "\n",
    "train_dataset = DehazingDataset(train_dir, transform)\n",
    "val_dataset = DehazingDataset(val_dir, transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "# Viewing clean and corresponding augmented images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_images(dataloader, num_images=5):\n",
    "    # Get a batch of data\n",
    "    data_iter = iter(dataloader)\n",
    "    _, images = next(data_iter)\n",
    "    \n",
    "    # Plot original clean images\n",
    "    fig, axes = plt.subplots(6, num_images, figsize=(15, 15))\n",
    "    for i in range(num_images):\n",
    "        clean_image = images[i].permute(1, 2, 0).cpu().numpy()  # Convert to NumPy array\n",
    "        clean_image = clean_image * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])  # Denormalize\n",
    "        \n",
    "        axes[0, i].imshow(clean_image)\n",
    "        axes[0, i].axis('off')\n",
    "        axes[0, i].set_title(\"Clean Image\")\n",
    "        \n",
    "        # Apply each augmentation function separately and plot the result\n",
    "        augmented_haze_image = add_haze(clean_image)\n",
    "        axes[1, i].imshow(augmented_haze_image)\n",
    "        axes[1, i].axis('off')\n",
    "        axes[1, i].set_title(\"Haze Only\")\n",
    "        \n",
    "        augmented_gaussian_haze_image = add_haze_gaussian(clean_image)\n",
    "        axes[2, i].imshow(augmented_gaussian_haze_image)\n",
    "        axes[2, i].axis('off')\n",
    "        axes[2, i].set_title(\"Haze with Gaussian Blur\")\n",
    "        \n",
    "        noisy_image = add_noise(clean_image)\n",
    "        axes[3, i].imshow(noisy_image)\n",
    "        axes[3, i].axis('off')\n",
    "        axes[3, i].set_title(\"Noisy Image\")\n",
    "        \n",
    "        atmospheric_scattering_image = add_atmospheric_scattering(clean_image)\n",
    "        axes[4, i].imshow(atmospheric_scattering_image)\n",
    "        axes[4, i].axis('off')\n",
    "        axes[4, i].set_title(\"Atmospheric Scattering\")\n",
    "        \n",
    "        # Apply all augmentation functions together\n",
    "        all_augmented_image = augment_hazy_image(clean_image)\n",
    "        axes[5, i].imshow(all_augmented_image)\n",
    "        axes[5, i].axis('off')\n",
    "        axes[5, i].set_title(\"All Augmentations Combined\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize images from train dataloader\n",
    "print(\"Images from Train Dataloader:\")\n",
    "show_images(train_dataloader)\n",
    "\n",
    "# Visualize images from validation dataloader\n",
    "print(\"Images from Validation Dataloader:\")\n",
    "show_images(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(dataloader, num_images=5):\n",
    "    # Get a batch of data\n",
    "    data_iter = iter(dataloader)\n",
    "    _, images = next(data_iter)\n",
    "    \n",
    "    # Plot original clean images\n",
    "    fig, axes = plt.subplots(6, num_images, figsize=(15, 15))\n",
    "    for i in range(num_images):\n",
    "        clean_image = images[i].permute(1, 2, 0).cpu().numpy()  # Convert to NumPy array\n",
    "        clean_image = clean_image * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])  # Denormalize\n",
    "        \n",
    "        \n",
    "        axes[0, i].imshow(clean_image)\n",
    "        axes[0, i].axis('off')\n",
    "        axes[0, i].set_title(\"Clean Image\")\n",
    "        \n",
    "        # Apply each augmentation function separately and plot the result\n",
    "        augmented_haze_image = add_haze(clean_image)\n",
    "        axes[1, i].imshow(augmented_haze_image)\n",
    "        axes[1, i].axis('off')\n",
    "        axes[1, i].set_title(\"Haze Only\")\n",
    "        \n",
    "        random_occlusion_image = add_random_occlusion(clean_image)\n",
    "        axes[2, i].imshow(random_occlusion_image)\n",
    "        axes[2, i].axis('off')\n",
    "        axes[2, i].set_title(\"Random Occlusion\")\n",
    "        \n",
    "        gradient_overlay_image = add_random_black_clouds(clean_image)\n",
    "        axes[3, i].imshow(gradient_overlay_image)\n",
    "        axes[3, i].axis('off')\n",
    "        axes[3, i].set_title(\"Black Cloud\")\n",
    "        \n",
    "        random_lighting_image = add_random_lighting(clean_image)\n",
    "        axes[4, i].imshow(random_lighting_image)\n",
    "        axes[4, i].axis('off')\n",
    "        axes[4, i].set_title(\"Random Lighting\")\n",
    "        \n",
    "        color_distorted_image = apply_color_distortion(clean_image)\n",
    "        axes[5, i].imshow(color_distorted_image)\n",
    "        axes[5, i].axis('off')\n",
    "        axes[5, i].set_title(\"Color Distortion\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize images from train dataloader\n",
    "print(\"Images from Train Dataloader:\")\n",
    "show_images(train_dataloader)\n",
    "\n",
    "# Visualize images from validation dataloader\n",
    "print(\"Images from Validation Dataloader:\")\n",
    "show_images(val_dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "# Viewing clean and corresponding hazy images from existing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(dataloader, num_images=5):\n",
    "    # Get a batch of data\n",
    "    data_iter = iter(dataloader)\n",
    "    hazy_images, clean_images = next(data_iter)\n",
    "    \n",
    "    # Denormalize images\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    \n",
    "    clean_images = clean_images.numpy().transpose((0, 2, 3, 1))\n",
    "    clean_images = std * clean_images + mean\n",
    "    clean_images = np.clip(clean_images, 0, 1)\n",
    "    \n",
    "    hazy_images = hazy_images.numpy().transpose((0, 2, 3, 1))\n",
    "    hazy_images = std * hazy_images + mean\n",
    "    hazy_images = np.clip(hazy_images, 0, 1)\n",
    "    \n",
    "    # Plot images\n",
    "    fig, axes = plt.subplots(num_images, 2, figsize=(15, num_images * 3))\n",
    "    for i in range(num_images):\n",
    "        axes[i, 0].imshow(clean_images[i])\n",
    "        axes[i, 0].set_title(\"Clean Image\")\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        axes[i, 1].imshow(hazy_images[i])\n",
    "        axes[i, 1].set_title(\"Hazy Image\")\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize images from train dataloader\n",
    "print(\"Images from Train Dataloader:\")\n",
    "show_images(train_dataloader)\n",
    "\n",
    "# Visualize images from validation dataloader\n",
    "print(\"Images from Validation Dataloader:\")\n",
    "show_images(val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
